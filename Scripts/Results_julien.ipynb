{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import folium\n",
    "import pickle\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file=\"./Data/uni_df.pkl\"\n",
    "df=pickle.load(open(pickle_file,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final features ['Sweet potatoes Crops Production tonnes', 'Soybeans Crops Production tonnes', 'Tomatoes Crops Production tonnes', 'Oats Crops Production tonnes', 'Buffaloes Livestock production Head', 'Cassava Crops Production tonnes', 'Barley Crops Production tonnes', 'Rabbits and hares Livestock production Head', 'Maize Crops Production tonnes', 'Goats Livestock production Head', 'Turkeys Livestock production Head', 'Ducks Livestock production Head', 'Sugar beet Crops Production tonnes', 'Potatoes Crops Production tonnes', 'Wheat Crops Production tonnes', 'Pigs Livestock production Head', 'Cattle Livestock production Head', 'Sheep Livestock production Head', 'Sheep and Goats Livestock production Head', 'Chickens Livestock production Head', 'Barley Food export quantities tonnes', 'Buffaloes Live animals export quantities Head', 'Cassava Food export quantities tonnes', 'Cattle Live animals export quantities Head', 'Chickens Live animals export quantities Head', 'Ducks Live animals export quantities Head', 'Goats Live animals export quantities Head', 'Maize Food export quantities tonnes', 'Maize, green Food export quantities tonnes', 'Oats Food export quantities tonnes', 'Pigs Live animals export quantities Head', 'Potatoes Food export quantities tonnes', 'Rabbits and hares Live animals export quantities Head', 'Sheep Live animals export quantities Head', 'Sheep and Goats Live animals export quantities Head', 'Sheep and Goats Live animals export quantities Head', 'Sheep and Goats Live animals export quantities Head', 'Soybeans Food export quantities tonnes', 'Sugar beet Food export quantities tonnes', 'Sweet potatoes Food export quantities tonnes', 'Tomatoes Food export quantities tonnes', 'Turkeys Live animals export quantities Head', 'Wheat Food export quantities tonnes', 'Barley Food import quantities tonnes', 'Buffaloes Live animals import quantities Head', 'Cassava Food import quantities tonnes', 'Cattle Live animals import quantities Head', 'Chickens Live animals import quantities Head', 'Ducks Live animals import quantities Head', 'Goats Live animals import quantities Head', 'Maize Food import quantities tonnes', 'Maize, green Food import quantities tonnes', 'Oats Food import quantities tonnes', 'Pigs Live animals import quantities Head', 'Potatoes Food import quantities tonnes', 'Rabbits and hares Live animals import quantities Head', 'Sheep Live animals import quantities Head', 'Sheep and Goats Live animals import quantities Head', 'Sheep and Goats Live animals import quantities Head', 'Sheep and Goats Live animals import quantities Head', 'Soybeans Food import quantities tonnes', 'Sugar beet Food import quantities tonnes', 'Sweet potatoes Food import quantities tonnes', 'Tomatoes Food import quantities tonnes', 'Turkeys Live animals import quantities Head', 'Wheat Food import quantities tonnes']\n",
      "list of all features ['Sweet potatoes Crops Production tonnes', 'Soybeans Crops Production tonnes', 'Tomatoes Crops Production tonnes', 'Oats Crops Production tonnes', 'Buffaloes Livestock production Head', 'Cassava Crops Production tonnes', 'Barley Crops Production tonnes', 'Rabbits and hares Livestock production Head', 'Maize Crops Production tonnes', 'Goats Livestock production Head', 'Turkeys Livestock production Head', 'Ducks Livestock production Head', 'Sugar beet Crops Production tonnes', 'Potatoes Crops Production tonnes', 'Wheat Crops Production tonnes', 'Pigs Livestock production Head', 'Cattle Livestock production Head', 'Sheep Livestock production Head', 'Sheep and Goats Livestock production Head', 'Chickens Livestock production Head', 'Barley Food export quantities tonnes', 'Buffaloes Live animals export quantities Head', 'Cassava Food export quantities tonnes', 'Cattle Live animals export quantities Head', 'Chickens Live animals export quantities Head', 'Ducks Live animals export quantities Head', 'Goats Live animals export quantities Head', 'Maize Food export quantities tonnes', 'Maize, green Food export quantities tonnes', 'Oats Food export quantities tonnes', 'Pigs Live animals export quantities Head', 'Potatoes Food export quantities tonnes', 'Rabbits and hares Live animals export quantities Head', 'Sheep Live animals export quantities Head', 'Sheep and Goats Live animals export quantities Head', 'Sheep and Goats Live animals export quantities Head', 'Sheep and Goats Live animals export quantities Head', 'Soybeans Food export quantities tonnes', 'Sugar beet Food export quantities tonnes', 'Sweet potatoes Food export quantities tonnes', 'Tomatoes Food export quantities tonnes', 'Turkeys Live animals export quantities Head', 'Wheat Food export quantities tonnes', 'Barley Food import quantities tonnes', 'Buffaloes Live animals import quantities Head', 'Cassava Food import quantities tonnes', 'Cattle Live animals import quantities Head', 'Chickens Live animals import quantities Head', 'Ducks Live animals import quantities Head', 'Goats Live animals import quantities Head', 'Maize Food import quantities tonnes', 'Maize, green Food import quantities tonnes', 'Oats Food import quantities tonnes', 'Pigs Live animals import quantities Head', 'Potatoes Food import quantities tonnes', 'Rabbits and hares Live animals import quantities Head', 'Sheep Live animals import quantities Head', 'Sheep and Goats Live animals import quantities Head', 'Sheep and Goats Live animals import quantities Head', 'Sheep and Goats Live animals import quantities Head', 'Soybeans Food import quantities tonnes', 'Sugar beet Food import quantities tonnes', 'Sweet potatoes Food import quantities tonnes', 'Tomatoes Food import quantities tonnes', 'Turkeys Live animals import quantities Head', 'Wheat Food import quantities tonnes']\n",
      "amount of selected features 15\n",
      "selected features ['Soybeans Crops Production tonnes', 'Tomatoes Crops Production tonnes', 'Maize Crops Production tonnes', 'Turkeys Livestock production Head', 'Chickens Livestock production Head', 'Maize Food export quantities tonnes', 'Maize, green Food export quantities tonnes', 'Soybeans Food export quantities tonnes', 'Wheat Food export quantities tonnes', 'Cattle Live animals import quantities Head', 'Oats Food import quantities tonnes', 'Pigs Live animals import quantities Head', 'Tomatoes Food import quantities tonnes', 'Turkeys Live animals import quantities Head', 'GDP']\n",
      "list of selected features after reduction ['Soybeans Crops Production tonnes', 'Tomatoes Crops Production tonnes', 'Maize Crops Production tonnes', 'Turkeys Livestock production Head', 'Maize Food export quantities tonnes', 'Maize, green Food export quantities tonnes', 'Wheat Food export quantities tonnes', 'Cattle Live animals import quantities Head', 'Oats Food import quantities tonnes', 'Pigs Live animals import quantities Head', 'Tomatoes Food import quantities tonnes', 'Turkeys Live animals import quantities Head']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/ada/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 88269505930363.5, tolerance: 616213877638.2041\n",
      "  positive)\n",
      "//anaconda3/envs/ada/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 52106333384857.0, tolerance: 641076863381.032\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best regularization parameter is  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/ada/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 25251481765867.0, tolerance: 650584559072.4933\n",
      "  positive)\n",
      "//anaconda3/envs/ada/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 25251481765867.0, tolerance: 650584559072.4933\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_target_and_covariate_df(path_to_pkl):\n",
    "    '''\n",
    "    path_to_pkl: path to the pickle file.\n",
    "    outputs two dataframes, one for the independant variables one for the dependant variables\n",
    "    '''\n",
    "    \n",
    "    uni_df = pd.read_pickle(path_to_pkl)\n",
    "    uni_df = uni_df.drop(columns=['Area', 'Year'])\n",
    "    target_variables_df = uni_df[['(GDP, million $)', '(Consumer price indices, %)']]\n",
    "    covariates_df = uni_df.drop(columns=['(GDP, million $)', '(Consumer price indices, %)'])\n",
    "    \n",
    "    return covariates_df, target_variables_df\n",
    "\n",
    "\n",
    "def drop_feature_pearson_correlation(threshold, target_variable, target_variable_name, dataframe):\n",
    "    \n",
    "    '''\n",
    "    threshold: the minimum amount of correlation required to keep the feature\n",
    "    target_variable_name: string GDP or CPI\n",
    "    normalised_dataset: the normalised dataset of feature\n",
    "    target_variable: pandas series that contains the value of the target_varibale_name\n",
    "    that we add to the normalised dataset\n",
    "    \n",
    "    '''\n",
    "    copy_dataframe = dataframe.copy()\n",
    "    copy_dataframe[target_variable_name] = target_variable\n",
    "    cor = copy_dataframe.corr()\n",
    "    cor_target = abs(cor[target_variable_name])\n",
    "    \n",
    "    relevant_features = cor_target[cor_target > threshold]\n",
    "    \n",
    "    return list(relevant_features.keys())\n",
    "\n",
    "def drop_too_corelated_featues(threshold, dataframe):\n",
    "    \n",
    "    corr_matrix = dataframe.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    \n",
    "    return dataframe.drop(dataframe[to_drop], axis=1)\n",
    "    \n",
    "\n",
    "def feature_augmentation(degree, covariates_df):\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    output_nparray =  poly.fit_transform(covariates_df)\n",
    "\n",
    "    \n",
    "    output_df = pd.DataFrame(output_nparray, columns = poly.get_feature_names(covariates_df.columns))\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "def split_and_standardization_dataset(target_variables, covariates, test_size, random, type_return = 'numpy'  ):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    target_variables: pandas dataframe that contains the target variables\n",
    "    covariates: pandas dataframe that contains the independant variables\n",
    "    test_size: the proportion of the dataset to include in the test split\n",
    "    type_return: 'numpy' if return numpy array, 'pandas' if return pandas dataframe\n",
    "    '''\n",
    "    target_variables_numpy = target_variables.to_numpy()\n",
    "    covariates_numpy = covariates.to_numpy()\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(covariates_numpy, target_variables_numpy, test_size=test_size, random_state = random)\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train_normalized = scaler.transform(X_train)\n",
    "    X_test_normalized = scaler.transform(X_test)\n",
    "    \n",
    "    if type_return == 'numpy':\n",
    "        \n",
    "        return X_train_normalized, X_test_normalized, Y_train, Y_test\n",
    "    \n",
    "    elif type_return == 'pandas':\n",
    "        \n",
    "        X_test_normalized_df = pd.DataFrame(X_test_normalized, columns = list(covariates.columns))\n",
    "        X_train_normalized_df = pd.DataFrame(X_train_normalized,columns= list(covariates.columns))\n",
    "        Y_train_df = pd.DataFrame(Y_train, columns= list(target_variables.columns))\n",
    "        Y_test_df = pd.DataFrame(Y_test, columns= list(target_variables.columns))\n",
    "        \n",
    "        return X_train_normalized_df, X_test_normalized_df, Y_train_df, Y_test_df\n",
    "\n",
    "def fit_model_lasso(regularisation_parameters, covariates_df, target_df, nb_fold_CV):\n",
    "    \n",
    "    lasso = Lasso()\n",
    "    \n",
    "    parameters = {'alpha': regularisation_parameters}\n",
    "    \n",
    "    lasso_regressor = GridSearchCV(lasso, parameters, scoring = 'neg_mean_squared_error', cv = nb_fold_CV)\n",
    "    lasso_regressor.fit(covariates_df, target_df)\n",
    "\n",
    "    best_param = lasso_regressor.best_params_['alpha']\n",
    "    print('The best regularization parameter is ', best_param)\n",
    "\n",
    "\n",
    "    lasso = Lasso(alpha=best_param)\n",
    "    lasso.fit(covariates_df, target_df)\n",
    "    return lasso.coef_\n",
    "    \n",
    "    \n",
    "    \n",
    "def RFECV_lasso_2(covariate, target,  random, nb_fold = 5,):\n",
    "    \n",
    "    cols = list(covariate.columns)\n",
    "    X_train_, X_test_, Y_train_, Y_test_ = split_and_standardization_dataset(target, covariate, 0.2, type_return='numpy', random = random)\n",
    "    #print('shape of Y_train_', Y_train_.shape, 'type of Y_train_', type(Y_train_))\n",
    "    model = Lasso()\n",
    "    \n",
    "    rfecv = RFECV(estimator = model, step = 1, cv = nb_fold, scoring = 'neg_mean_squared_error')\n",
    "    rfecv.fit(X_train_, np.ravel(Y_train_))\n",
    "    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "    \n",
    "    temp = pd.Series(rfecv.support_,index = cols)\n",
    "    selected_features = temp[temp==True].index\n",
    "\n",
    "    print(selected_features)\n",
    "    \n",
    "\n",
    "    # plt.figure()\n",
    "    # plt.xlabel(\"Number of features selected\")\n",
    "    # plt.ylabel(\"Cross validation score\")\n",
    "    # plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "    # plt.show()\n",
    "        \n",
    "    return selected_features\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def main():\n",
    "\n",
    "    RANDOM_SEED = 29\n",
    "\n",
    "    params = {\n",
    "\n",
    "        'target' : '(GDP, million $)',\n",
    "        'name of target': 'GDP',\n",
    "        'pearson correlation threshold': 0.4,\n",
    "        'inter correlation threshold': 0.9, \n",
    "        'nb_fold_CV': 5, \n",
    "        'degree augmentation': 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "    covariates_df, target_variables_df = create_target_and_covariate_df('./Data/uni_df.pkl')\n",
    "    target_variables_df.to_pickle('target.pkl')\n",
    "\n",
    "    ### Below we are going to select the top 20 features in production:\n",
    "\n",
    "    Production_cov_df = covariates_df.filter(regex= 'production|Production')\n",
    "    summed_df = Production_cov_df.sum()\n",
    "    keys = summed_df.keys()\n",
    "    values = summed_df.values\n",
    "    sorted_keys = [key for _,key in sorted(zip(values,keys))]\n",
    "    Production_cov_df = Production_cov_df[sorted_keys[-20:]]\n",
    "    selected_features_production = list(Production_cov_df.columns.values) # Selected features for top 20 prod features in volumne\n",
    "\n",
    "    cropped_word_selected_prod = [\" \".join(string.split()[:-3]) for string in selected_features_production] # Same as the list above with only the important words kept\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "   #-------------------------Below we are selecting the features in export that have been selected previously with the production--------------------------------------\n",
    "    export_df = covariates_df.filter(regex= 'export')\n",
    "    \n",
    "\n",
    "    columns_to_keep_export = []\n",
    "\n",
    "    for column_export in list(export_df.columns.values):\n",
    "\n",
    "        for columns_prod in cropped_word_selected_prod:\n",
    "\n",
    "            if columns_prod in column_export:\n",
    "\n",
    "                columns_to_keep_export.append(column_export)\n",
    "\n",
    "\n",
    "    #-------------------------Below we are selecting the features in import that have been selected previously with the production--------------------------------------\n",
    "    import_df = covariates_df.filter(regex= 'import')\n",
    "    \n",
    "\n",
    "    columns_to_keep_import = []\n",
    "\n",
    "    for column_import in list(import_df.columns.values):\n",
    "\n",
    "        for columns_prod in cropped_word_selected_prod:\n",
    "\n",
    "            if columns_prod in column_import:\n",
    "\n",
    "                columns_to_keep_import.append(column_import)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    final_features_kept = selected_features_production + columns_to_keep_export + columns_to_keep_import  # All the selected features\n",
    "    print('final features', final_features_kept)\n",
    "\n",
    "    # summed_covariates_df = covariates_df.sum()\n",
    "    # keys = summed_covariates_df.keys()\n",
    "    # values = summed_covariates_df.values\n",
    "    \n",
    "    # sorted_keys = [key for _,key in sorted(zip(values,keys))]\n",
    "\n",
    "    # covariates_df = covariates_df[sorted_keys[-30:]]\n",
    "    covariates_df = covariates_df[final_features_kept]\n",
    "    \n",
    "    # covariates_df = feature_augmentation(2, covariates_df)\n",
    "    print('list of all features', list(covariates_df.columns.values))\n",
    "    list_selected_features_GDP = drop_feature_pearson_correlation(params['pearson correlation threshold'], target_variables_df[params['target']], params['name of target'], covariates_df)\n",
    "    print('amount of selected features', len(list_selected_features_GDP))\n",
    "    print('selected features', list_selected_features_GDP)\n",
    "    covariate_reduced_df = covariates_df[list_selected_features_GDP[:-1]]\n",
    "\n",
    "    covariate_reduced_df = drop_too_corelated_featues(params['inter correlation threshold'], covariate_reduced_df)\n",
    "    covariate_reduced_df.to_pickle(\"reduced_df_2.pkl\")\n",
    "    print('list of selected features after reduction', list(covariate_reduced_df.columns.values))\n",
    "    # covariate_reduced_df = feature_augmentation(params['degree augmentation'], covariate_reduced_df)\n",
    "\n",
    "    # selected_features = RFECV_lasso_2(covariate_reduced_df, target_variables_df[[params['target']]], random = RANDOM_SEED)\n",
    "    # selected_covariate = covariate_reduced_df[selected_features]\n",
    "\n",
    "    regularisation_parameters = np.linspace(start = 0.01, stop= 1, num = 20)\n",
    "\n",
    "    # covariate_reduced_df = covariate_reduced_df[list(selected_covariate.columns.values)]\n",
    "\n",
    "    target_df = target_variables_df[params['target']]\n",
    "\n",
    "    nb_fold_CV = params['nb_fold_CV']\n",
    "\n",
    "    param_lasso = fit_model_lasso(regularisation_parameters, covariate_reduced_df, target_df, nb_fold_CV = nb_fold_CV )\n",
    "\n",
    "    keys = list(covariate_reduced_df.columns.values)\n",
    "    #keys = selected_features\n",
    "    values = param_lasso\n",
    "   \n",
    "    return dict(zip(keys, values))\n",
    "\n",
    "weights=main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "\n",
    "def visualise_world_data_folium(df, to_visualise, year, log=True,log2=False):\n",
    "    \n",
    "    if log2:\n",
    "        log=False\n",
    "    if log:\n",
    "        log2=False\n",
    "        \n",
    "    # Defining color palette\n",
    "    color_scale = sns.cubehelix_palette(9)\n",
    "    \n",
    "    # importing geojson and transforming to pandas\n",
    "    geo_data=json.load(open(\"./Data/world-countries.json\"))\n",
    "    dics=geo_data['features']\n",
    "    clean_dics=[]\n",
    "    for country in dics:\n",
    "        clean_dics.append({'Country':country['properties']['name'],\n",
    "                          'geometry':country['geometry']})\n",
    "    geo_df=pd.DataFrame(clean_dics)\n",
    "    \n",
    "    # cropping to df to data of interest\n",
    "    df_visu=df[df.Year==year][['Area',to_visualise]]\n",
    "\n",
    "    # Merging with geo data\n",
    "    df_visu=geo_df.merge(df_visu,how='left',left_on='Country',right_on='Area')\n",
    "    df_visu=df_visu.dropna()\n",
    "    \n",
    "    if log:\n",
    "        df_visu['to_plot']=df_visu[to_visualise].apply(lambda x : np.log10(x))\n",
    "        \n",
    "    def log2_scale(x):\n",
    "        out=np.sign(x)*np.log10(1+np.abs(x))\n",
    "        return out\n",
    "        \n",
    "    if log2:\n",
    "        df_visu['to_plot']=df_visu[to_visualise].apply(log2_scale)\n",
    "    \n",
    "    # creating bins for color scaling\n",
    "    ma_value=df_visu['to_plot'].max()\n",
    "    mi_value=df_visu['to_plot'].min()\n",
    "    bins=np.linspace(mi_value,ma_value,8)\n",
    "    \n",
    "    # creating Json string for folium\n",
    "    features=[]\n",
    "    for _,row in df_visu.iterrows():\n",
    "        color=np.digitize(row['to_plot'],bins)\n",
    "        val=row[to_visualise]\n",
    "        feature={\n",
    "            'type' : 'Feature',\n",
    "            \n",
    "            'properties':{'name':row['Country'],\n",
    "                          'value': '{:.2E}'.format(val),\n",
    "                          'color':colors.to_hex(color_scale[color])},\n",
    "            'geometry':row['geometry']\n",
    "            }\n",
    "        features.append(feature)\n",
    "    \n",
    "    def style(feature):\n",
    "        \n",
    "        if feature['properties']['value']==np.nan:\n",
    "            print(\"lol\")\n",
    "            opac=0\n",
    "        else:\n",
    "            opac=0.8\n",
    "        return {'fillOpacity':opac,\n",
    "                   'weight':0.1,\n",
    "                   'fillColor':feature['properties']['color']}\n",
    "    geo_data=folium.GeoJson({'type':'FeatureCollection','features':features},\n",
    "                            style_function=style,\n",
    "                            tooltip=folium.features.GeoJsonTooltip(['name','value']))\n",
    "    m=folium.Map()\n",
    "    geo_data.add_to(m)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Soybeans Crops Production tonnes': 0.016904551719883004,\n",
       " 'Tomatoes Crops Production tonnes': 0.04600299893471108,\n",
       " 'Maize Crops Production tonnes': 0.019112555979080598,\n",
       " 'Turkeys Livestock production Head': 0.00045609049200977524,\n",
       " 'Maize Food export quantities tonnes': -0.06316454222878906,\n",
       " 'Maize, green Food export quantities tonnes': 52.082838493928676,\n",
       " 'Wheat Food export quantities tonnes': 0.0075799594660751575,\n",
       " 'Cattle Live animals import quantities Head': 0.03663454785851955,\n",
       " 'Oats Food import quantities tonnes': 0.48376570716305617,\n",
       " 'Pigs Live animals import quantities Head': 0.05776106945901765,\n",
       " 'Tomatoes Food import quantities tonnes': 2.5686557773141767,\n",
       " 'Turkeys Live animals import quantities Head': 0.04929538371588062}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=list(weights.keys())\n",
    "df=pickle.load(open(pickle_file,'rb'))\n",
    "df=df.set_index(['Area','Year'])\n",
    "prod_to_plot=pd.DataFrame(index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soybeans Crops Production tonnes\n",
      "Tomatoes Crops Production tonnes\n",
      "Maize Crops Production tonnes\n",
      "Turkeys Livestock production Head\n",
      "Maize Food export quantities tonnes\n",
      "Maize, green Food export quantities tonnes\n",
      "Wheat Food export quantities tonnes\n",
      "Cattle Live animals import quantities Head\n",
      "Oats Food import quantities tonnes\n",
      "Pigs Live animals import quantities Head\n",
      "Tomatoes Food import quantities tonnes\n",
      "Turkeys Live animals import quantities Head\n"
     ]
    }
   ],
   "source": [
    "dic_to_plot={}\n",
    "for c in columns:\n",
    "    print(c)\n",
    "    if 'Production' in c or 'production' in c:\n",
    "        if len(df.filter(regex=c).columns)==0:\n",
    "            print('{} not found'.format(c))\n",
    "        else:\n",
    "            dic_to_plot.update(df.filter(regex=c).to_dict())\n",
    "    else:\n",
    "        s=re.split(' Food|Live ',c)[0]\n",
    "        s='^'+s\n",
    "        s=(s+'.*Production.*tonnes$|'\n",
    "           +s+'.*Production.*Head$|'\n",
    "           +s+'.*production.*tonnes$|'\n",
    "           +s+'.*production.*Head')\n",
    "        if len(df.filter(regex=s).columns)==0:\n",
    "            print('{} not found'.format(c))\n",
    "        else:\n",
    "            dic_to_plot.update(df.filter(regex=s).to_dict())\n",
    "prod_to_plot=pd.DataFrame(dic_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Soybeans Crops Production tonnes</th>\n",
       "      <th>Tomatoes Crops Production tonnes</th>\n",
       "      <th>Maize Crops Production tonnes</th>\n",
       "      <th>Turkeys Livestock production Head</th>\n",
       "      <th>Maize, green Crops Production tonnes</th>\n",
       "      <th>Wheat Crops Production tonnes</th>\n",
       "      <th>Cattle Livestock production Head</th>\n",
       "      <th>Oats Crops Production tonnes</th>\n",
       "      <th>Pigs Livestock production Head</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>667000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2081000.0</td>\n",
       "      <td>3700000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Soybeans Crops Production tonnes  \\\n",
       "Afghanistan 1970                               0.0   \n",
       "\n",
       "                  Tomatoes Crops Production tonnes  \\\n",
       "Afghanistan 1970                               0.0   \n",
       "\n",
       "                  Maize Crops Production tonnes  \\\n",
       "Afghanistan 1970                       667000.0   \n",
       "\n",
       "                  Turkeys Livestock production Head  \\\n",
       "Afghanistan 1970                                0.0   \n",
       "\n",
       "                  Maize, green Crops Production tonnes  \\\n",
       "Afghanistan 1970                                   0.0   \n",
       "\n",
       "                  Wheat Crops Production tonnes  \\\n",
       "Afghanistan 1970                      2081000.0   \n",
       "\n",
       "                  Cattle Livestock production Head  \\\n",
       "Afghanistan 1970                         3700000.0   \n",
       "\n",
       "                  Oats Crops Production tonnes  Pigs Livestock production Head  \n",
       "Afghanistan 1970                           0.0                             0.0  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_to_plot.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_to_plot=prod_to_plot.reset_index().rename(columns={'level_0':'Area','level_1':'Year'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for c in prod_to_plot.columns:\n",
    "    if c!='Year' and c!='Area':\n",
    "        os.mkdir('./Data/ResultsJulien/Producers/{}'.format(c))\n",
    "        for year in range(1970,2015,1):\n",
    "            m=visualise_world_data_folium(prod_to_plot,c,year,log2=True)\n",
    "            save_name='./Data/ResultsJulien/Producers/{}/{}_{}.html'.format(c,c,year)\n",
    "            m.save(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_self_suficiency(df,weights=None):\n",
    "    \n",
    "    # From the unified dataframe df, compute the self sufficiency score for each year for each country\n",
    "    # if a paramter of weights is given as a dict, the method returns the aggregated score.\n",
    "    \n",
    "    \n",
    "    #Useful method to manipulate names\n",
    "    def drop_words( s , w=1 , end=True):\n",
    "        if end:\n",
    "            return s.rsplit(' ',w)[0]\n",
    "        else:\n",
    "            return s.split(' ',w)[-1]\n",
    "    \n",
    "    df=df.set_index(['Area','Year'])\n",
    "    \n",
    "    #Getting the columns corresponding to import, export and production\n",
    "    import_cols=[col for col in df.columns if 'import' in col.lower()]\n",
    "    export_cols=[col for col in df.columns if 'export' in col.lower()]\n",
    "    prod_cols=[col for col in df.columns if 'production' in col.lower()]\n",
    "    \n",
    "    #Initializing new dataframe\n",
    "    scores=pd.DataFrame(index=df.index)\n",
    "    #Dict with columns names\n",
    "    columns={}\n",
    "    \n",
    "    #Generating scores\n",
    "    for i,col in enumerate(import_cols):\n",
    "        key=col+prod_cols[i]+export_cols[i]\n",
    "        scores[key]=(df[prod_cols[i]]*100/(\n",
    "                                    df[prod_cols[i]]+df[import_cols[i]]-df[export_cols[i]]))\n",
    "        columns[key]=drop_words(col,3)\n",
    "        \n",
    "    #If no weights, return scores without aggregate\n",
    "    if weights==None:\n",
    "        scores=scores.rename(columns=columns)\n",
    "        return scores\n",
    "    \n",
    "    features=[w for w in weights.keys()]\n",
    "    temp=pd.DataFrame(index=df.index)\n",
    "    \n",
    "    #replacing na with 0 to avoid na aggregated scores\n",
    "    scores=scores.fillna(0)\n",
    "    \n",
    "    #Selecting features of interest\n",
    "    temp_dic={}\n",
    "    for feat in features:\n",
    "        temp_dic.update(scores.filter(regex=feat))\n",
    "    temp=pd.DataFrame(temp_dic)\n",
    "    temp.columns=features\n",
    "    \n",
    "    #Aggregating the scores\n",
    "    temp['Agg']=0\n",
    "    for feat in features:\n",
    "        temp['Agg']=temp['Agg']+weights[feat]*temp[]\n",
    "    scores=temp['Agg']\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pickle.load(open(pickle_file,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 9 elements, new values have 12 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f7e311c92ae4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_self_suficiency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-c09c1f165450>\u001b[0m in \u001b[0;36mcompute_self_suficiency\u001b[0;34m(df, weights)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtemp_dic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m#Aggregating the scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/ada/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5191\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5192\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5193\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5194\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/ada/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/ada/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    181\u001b[0m             raise ValueError(\n\u001b[1;32m    182\u001b[0m                 \u001b[0;34m\"Length mismatch: Expected axis has {old} elements, new \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0;34m\"values have {new} elements\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mold_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             )\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 9 elements, new values have 12 elements"
     ]
    }
   ],
   "source": [
    "scores=compute_self_suficiency(df,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
